<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SSL | MIRer</title>
    <link>/tags/ssl/</link>
      <atom:link href="/tags/ssl/index.xml" rel="self" type="application/rss+xml" />
    <description>SSL</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 14 Jun 2025 18:24:35 +0100</lastBuildDate>
    <image>
      <url>/images/icon_hu25a3ac0bff28d67894e084ca1bc4b864_11852_512x512_fill_lanczos_center_2.png</url>
      <title>SSL</title>
      <link>/tags/ssl/</link>
    </image>
    
    <item>
      <title>CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following</title>
      <link>/publication/cmi-bench/</link>
      <pubDate>Sat, 14 Jun 2025 18:24:35 +0100</pubDate>
      <guid>/publication/cmi-bench/</guid>
      <description>&lt;p&gt;Abstract: Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking: reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2506.12285&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nicolaus625/CMI-bench&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/nicolaus625/CMI-bench&#34; target=&#34;_blank&#34;&gt;Dataset link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the 26th International Society for Music Information Retrieval (ISMIR 2025).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Audio-flan: A preliminary release</title>
      <link>/publication/audio-flan/</link>
      <pubDate>Sun, 23 Feb 2025 18:24:35 +0100</pubDate>
      <guid>/publication/audio-flan/</guid>
      <description>&lt;p&gt;Abstract: 
Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supergpqa: Scaling llm evaluation across 285 graduate disciplines</title>
      <link>/publication/supergpqa/</link>
      <pubDate>Thu, 20 Feb 2025 18:24:35 +0100</pubDate>
      <guid>/publication/supergpqa/</guid>
      <description>&lt;p&gt;Abstract: 
Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Foundation models for music: A survey</title>
      <link>/publication/fm4music/</link>
      <pubDate>Mon, 26 Aug 2024 18:24:35 +0100</pubDate>
      <guid>/publication/fm4music/</guid>
      <description>&lt;p&gt;Abstract: In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music …&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nicolaus625/FM4Music&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Huggingface daily paper top 3, 27 Aug 2024.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Map-neo: Highly capable and transparent bilingual large language model series</title>
      <link>/publication/neo/</link>
      <pubDate>Wed, 29 May 2024 18:24:35 +0100</pubDate>
      <guid>/publication/neo/</guid>
      <description>&lt;p&gt;Abstract: Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model&#39;s weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our …&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bridging Music &amp; Text with Pre-trained Models for Music Captioning and QA</title>
      <link>/project/musilingo/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/musilingo/</guid>
      <description>&lt;p&gt;07/2023 – present&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed Music Instruct (MI) query-response dataset based on captions &amp;amp; well-designed prompts to GPT-4.&lt;/li&gt;
&lt;li&gt;Achieved cutting-edge performance in question answering on both MusicQA and Music Instruct datasets.&lt;/li&gt;
&lt;li&gt;Employed instruct fine-tuning techniques on MI to attain state-of-the-art (SOTA) results in captioning.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/project/mert/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/mert/</guid>
      <description>&lt;p&gt;08/2022 – 05/2023&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Built self-supervised learning systems, acquiring 50k+ downloading of checkpoints on Huggingface.&lt;/li&gt;
&lt;li&gt;Replaced the pseudo-tag from MFCCs to Chroma music features for harmonic information.&lt;/li&gt;
&lt;li&gt;Utilising deep features like Encodec instead of k-means for scaling up models to 1 B parameters.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Domain-specific continual pre-training scaling law for large language models</title>
      <link>/publication/d-cpt/</link>
      <pubDate>Tue, 19 Sep 2023 18:24:35 +0100</pubDate>
      <guid>/publication/d-cpt/</guid>
      <description>&lt;p&gt;Abstract: Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely used to expand the model’s fundamental understanding of specific downstream domains (eg, math and code). For the CPT on domain-specific LLMs, one important question is how to choose the optimal mixture ratio between the general-corpus (eg, Dolma, Slim-pajama) and the downstream domain-corpus. Existing methods usually adopt laborious human efforts by grid-searching on a set of mixture ratios, which require high GPU training consumption costs. Besides, we cannot guarantee the selected ratio is optimal for the specific domain. To address the limitations of existing methods, inspired by the Scaling Law for performance prediction, we propose to investigate the Scaling Law of the Domain-specific Continual Pre-Training (D-CPT Law) to decide the optimal mixture ratio with acceptable training costs for LLMs of different sizes. Specifically, by fitting the D-CPT Law, we can easily predict the general and downstream performance of arbitrary mixture ratios, model sizes, and dataset sizes using small-scale training costs on limited experiments. Moreover, we also extend our standard D-CPT Law on cross-domain settings and propose the Cross-Domain D-CPT Law to predict the D-CPT law of target domains, where very small training costs (about 1% of the normal training costs) are needed for the target domains. Comprehensive experimental results on six downstream domains demonstrate the effectiveness and generalizability of our proposed D-CPT Law and Cross-Domain D-CPT Law.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/a4628e9fbd3002a554923642f74d5d6b-Paper-Conference.pdf&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response</title>
      <link>/publication/musilingo/</link>
      <pubDate>Tue, 19 Sep 2023 18:24:35 +0100</pubDate>
      <guid>/publication/musilingo/</guid>
      <description>&lt;p&gt;Abstract: Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.08730.pdf&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zihaod/musilingo&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/m-a-p/Music-Instruct/tree/main&#34; target=&#34;_blank&#34;&gt;Dataset link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2024).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Yue: Scaling open foundation models for long-form music generation</title>
      <link>/publication/yue/</link>
      <pubDate>Tue, 19 Sep 2023 18:24:35 +0100</pubDate>
      <guid>/publication/yue/</guid>
      <description>&lt;p&gt;Abstract: We tackle the task of long-form music generation&amp;ndash;particularly the challenging \textbf{lyrics-to-song} problem&amp;ndash;by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE&#39;s learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/publication/mert/</link>
      <pubDate>Wed, 31 May 2023 18:24:35 +0100</pubDate>
      <guid>/publication/mert/</guid>
      <description>&lt;p&gt;Abstract: Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). These teachers effectively guide our student model, a BERT-style transformer encoder, to better model music audio. In addition, we introduce an in-batch noise mixture augmentation to enhance the representation robustness. Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attains state-of-the-art (SOTA) overall scores. The code …&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00107&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/yizhilll/MERT&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rejected by NeurIPS2023&lt;/li&gt;
&lt;li&gt;Submitted to ICLR2024&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>On the effectiveness of speech self-supervised learning for musicCCF none</title>
      <link>/publication/ismir2023ssl/</link>
      <pubDate>Fri, 14 Apr 2023 18:24:35 +0100</pubDate>
      <guid>/publication/ismir2023ssl/</guid>
      <description>&lt;p&gt;Abstract: Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent speech models such as wav2vec2.0 have shown promise in music modelling. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train  SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.05161.pdf&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Accepted by ISMIR 2023, Milan, Italy.&lt;/li&gt;
&lt;li&gt;Music2Vec can be founded at here, the link is the same as our ISMIR2022LBD demo &lt;a href=&#34;https://huggingface.co/m-a-p/music2vec-v1&#34;&gt;https://huggingface.co/m-a-p/music2vec-v1&lt;/a&gt; and the training code is the same as the MERT training code. But the k-means feature need to be replaced with MFCC as the information described in the paper &lt;a href=&#34;https://github.com/yizhilll/MERT&#34;&gt;https://github.com/yizhilll/MERT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Large-Scale Pretrained Model for Self-Supervised Music Audio Representation Learning</title>
      <link>/publication/dmrn2022/</link>
      <pubDate>Mon, 19 Dec 2022 18:24:35 +0100</pubDate>
      <guid>/publication/dmrn2022/</guid>
      <description>&lt;p&gt;Abstract: Self-supervised learning technique is an under-explored topic for music audio due to the challenge of designing an appropriate training paradigm. We hence propose MAP-MERT, a large-scale music audio pre-trained model for general music understanding. We achieve performance that is comparable to the state-of-the-art pre-trained model Jukebox using less than 2% of parameters.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://qmro.qmul.ac.uk/xmlui/handle/123456789/83372&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Presented by DMRN workshop 2022.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Map-music2vec: A simple and effective baseline for self-supervised music audio representation learning</title>
      <link>/publication/music2vec/</link>
      <pubDate>Mon, 05 Dec 2022 18:24:35 +0100</pubDate>
      <guid>/publication/music2vec/</guid>
      <description>&lt;p&gt;Abstract: The deep learning community has witnessed an exponentially growing interest in self-supervised learning (SSL). However, it still remains unexplored how to build a framework for learning useful representations of raw music waveforms in a self-supervised manner. In this work, we design Music2Vec, a framework exploring different SSL algorithmic components and tricks for music audio recordings. Our model achieves comparable results to the state-of-the-art (SOTA) music SSL model Jukebox, despite being significantly smaller with less than 2% of parameters of the latter. The model will be released on Huggingface.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.02508&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/m-a-p/music2vec-v1&#34; target=&#34;_blank&#34;&gt;Huggingface checkpoints.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accepted by ISMIR 2022, late breaking demo.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/project/tmnn/</link>
      <pubDate>Mon, 29 Aug 2022 21:40:17 +0100</pubDate>
      <guid>/project/tmnn/</guid>
      <description>&lt;p&gt;09/2021 – 07/2022&lt;/p&gt;
&lt;p&gt;Research Assistant, Supervised by Prof. Richard Stern, Carnegie Mellon University&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constructed 2-layer learnable front ends in Temporal Modulation Neural Network (TMNN) that combines
Mel-like data-driven front ends and temporal modulation filters.&lt;/li&gt;
&lt;li&gt;Examined the proposed front ends surpass state-of-the-art (SOTA) methods on the MagnaTagATune dataset in
automatic music tagging, and they are also helpful for keyword spotting on speech commands.&lt;/li&gt;
&lt;li&gt;Analysis of the model performance among tags with different genres and instrument tags.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
