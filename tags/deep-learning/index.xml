<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | MIRer</title>
    <link>/tags/deep-learning/</link>
      <atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 24 Oct 2025 11:29:43 +0800</lastBuildDate>
    <image>
      <url>/images/icon_hu25a3ac0bff28d67894e084ca1bc4b864_11852_512x512_fill_lanczos_center_2.png</url>
      <title>Deep Learning</title>
      <link>/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>ğŸ‰ğŸ‰ğŸ‰Awarded Google PhD Fellowship 2025</title>
      <link>/publication/googlefellowship/</link>
      <pubDate>Fri, 24 Oct 2025 11:29:43 +0800</pubDate>
      <guid>/publication/googlefellowship/</guid>
      <description>&lt;p&gt;We are extremely proud to announce that Yinghao MA, a PhD student in AI and music at the Centre for Digital Music at QMUL, has been awarded the 2025 Google Fellowship in Machine Perception.&lt;/p&gt;
&lt;p&gt;The Google PhD Fellowship Program was created to recognise outstanding graduate students doing exceptional and innovative research in areas relevant to computer science and related fields.&lt;/p&gt;
&lt;p&gt;A Google spokesperson said: â€œThe student nominations we received this year were exemplary in their quality, but Yinghao especially stood out and was endorsed by the research scientists and distinguished engineers within Google who participated in the review. Congratulations to Yinghao on this well-deserved recognition, itâ€™s an honor to support such incredibly talented students.â€&lt;/p&gt;
&lt;p&gt;Yinghao Ma&#39;s PhD research focuses on advancing Large Language Models (LLMs) for music understanding and generation. Specifically, he studies how multimodal models can integrate audio, symbolic, and textual information to understand, reason about, and generate music.&lt;/p&gt;
&lt;p&gt;Together with colleagues, he developed MERT, a large-scale music audio representation model which has more than 10k monthly download in the past three years. His recent work includes developing music instruction-following datasets and benchmarks that help evaluate how well AI systems can comprehend and create music.&lt;/p&gt;
&lt;p&gt;He said: &amp;ldquo;It&#39;s my great honour to receive the Google PhD Fellowship that recognises my research and strongly contribute to my future career. Iâ€™m deeply grateful to Google and QMUL for the support, providing good platforms for AI &amp;amp; music research.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#39;https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-student-awarded-google-phd-fellowship.html&#39; target=&#39;_blank&#39;&gt;News @ QMUL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#39;https://www.linkedin.com/feed/update/urn:li:activity:7387160161896939521/&#39; target=&#39;_blank&#39;&gt;Linkedin post @ QMUL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#39;https://research.google/programs-and-events/phd-fellowship/recipients/&#39; target=&#39;_blank&#39;&gt;complete list of fellowship recipients on Google&#39;s website.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#39;https://blog.google/outreach-initiatives/google-org/phd-fellowship-program-2025/&#39; target=&#39;_blank&#39;&gt;News @ Google&#39;s blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/project/mert/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/mert/</guid>
      <description>&lt;p&gt;08/2022 â€“ 05/2023&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Built self-supervised learning systems, acquiring 50k+ downloading of checkpoints on Huggingface.&lt;/li&gt;
&lt;li&gt;Replaced the pseudo-tag from MFCCs to Chroma music features for harmonic information.&lt;/li&gt;
&lt;li&gt;Utilising deep features like Encodec instead of k-means for scaling up models to 1 B parameters.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/project/tmnn/</link>
      <pubDate>Mon, 29 Aug 2022 21:40:17 +0100</pubDate>
      <guid>/project/tmnn/</guid>
      <description>&lt;p&gt;09/2021 â€“ 07/2022&lt;/p&gt;
&lt;p&gt;Research Assistant, Supervised by Prof. Richard Stern, Carnegie Mellon University&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constructed 2-layer learnable front ends in Temporal Modulation Neural Network (TMNN) that combines
Mel-like data-driven front ends and temporal modulation filters.&lt;/li&gt;
&lt;li&gt;Examined the proposed front ends surpass state-of-the-art (SOTA) methods on the MagnaTagATune dataset in
automatic music tagging, and they are also helpful for keyword spotting on speech commands.&lt;/li&gt;
&lt;li&gt;Analysis of the model performance among tags with different genres and instrument tags.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learnable Frontend for Music, Speech and Audiow</title>
      <link>/project/masterthesis/</link>
      <pubDate>Tue, 02 Nov 2021 22:05:06 -0400</pubDate>
      <guid>/project/masterthesis/</guid>
      <description>&lt;p&gt;Design learnable frontends for deep learning models inspired by classic filters, multi-rate sampling &amp;amp; modulation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>åŸºäºæœºå™¨å­¦ä¹ çš„ç¬›å­æ¼”å¥æŠ€æ³• è¯†åˆ«ç ”ç©¶ä¸å®ç° (Research &amp; implementation of Chinese flute playing technique recognition based on ML)</title>
      <link>/publication/graduation/</link>
      <pubDate>Wed, 02 Sep 2020 11:29:43 +0800</pubDate>
      <guid>/publication/graduation/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Feb. 2020 â€“ May 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;One of the graduation theses that awarded the outstanding &lt;a href=&#39;../../files/æ¯•è®¾è®ºæ–‡.pdf&#39; target=&#39;_blank&#39;&gt;paper&lt;/a&gt; honor of School of Mathematical Science, Peking University.&lt;/li&gt;
&lt;li&gt;Supervised by prof. CHEN Xiaoou in Wangxuan Institute of Computer Technology, PKU.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;see at project for more information.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tempo Detection of Chinese Pop Music</title>
      <link>/project/summer_deepmusic_mir/</link>
      <pubDate>Wed, 02 Sep 2020 00:21:40 +0800</pubDate>
      <guid>/project/summer_deepmusic_mir/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Jun. 2020 &amp;ndash; Sept. 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;Summer internship in Beijing Deepmusic Technology Co. LTD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Write literature review on song tempo/speed detection.&lt;/p&gt;
&lt;p&gt;Designing new model on tempo detection based on BiLSTM and Temporal Convolution Network(TCN) and compared them with the baselines of Librosa and MadMOM using the data provided by Renren Karaoke Company (more than 2000 songs manually marked by my colleagues).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the music with stable speed or with or a slightly slower ending, the accuracy of tempo recognition is above 87% without considering the double frequency (error is less than or equal to 0.01) and above 98% general accuracy (error is less than 0.1). While MadMOM less than 90%.&lt;/li&gt;
&lt;li&gt;The accuracy in distinguishing between three beats(three four beats, six eight beats, twelvw eight beats etc.) and four beats(four four beats etc.) is 95 percent.&lt;/li&gt;
&lt;li&gt;The local tempo tasks such as song&#39;s inherent shift and whether the ending slows down deserve more attention&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Research &amp; implementation of Chinese flute playing technique recognition based on machine learning</title>
      <link>/project/graduate/</link>
      <pubDate>Wed, 02 Sep 2020 00:21:10 +0800</pubDate>
      <guid>/project/graduate/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Feb. 2020 â€“ May 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;One of the graduation theses that awarded the outstanding &lt;a href=&#39;../../files/æ¯•è®¾è®ºæ–‡.pdf&#39; target=&#39;_blank&#39;&gt;paper&lt;/a&gt; honor of School of Mathematical Science, Peking University.&lt;/li&gt;
&lt;li&gt;Research Assistant for prof. CHEN Xiaoou in Wangxuan Institute of Computer Technology at Peking University.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Music object recognition and recording is the essential component of music information retrieval. Different from other fields of melody extraction and music transcription, the research on musical instrument technique detection is still in the early stage. The existing work mainly focuses on technique detection of individual notes or frame, and there is a lack of effective data set.&lt;/p&gt;
&lt;p&gt;This article constructed &lt;a href=&#39;https://pan.baidu.com/s/1Czf6ZYqkW1EEpZeJh_3_gw&#39; target=&#39;_blank&#39;&gt;audio dataset&lt;/a&gt;(extraction code &amp;ldquo;jvbk&amp;rdquo; if needed) on ten kinds of techniques of transverse Chinese bamboo flute (Di), with 101 minutes of audio, using Mel frequency spectrum as audio feature, and put forward a model based on the fully convolutional neural network (FCNN). This model is an end-to-end sound event detector for variable length of the input and can be used in the detection of
instruments technology.&lt;/p&gt;
&lt;p&gt;Compared to the model based on VGG13, VGG16 and LeNet-5 baseline to evaluate the effectiveness of the proposed framework, this model got the average accuracy 94%, much better than all the others, on input of the different length of audio, which suggest good generalization ability.&lt;/p&gt;
&lt;p&gt;Key Wordsï¼š sound event detection, music information retrieval, fully convolutional neural network, musical instrumet technique detection&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chinese instrument recognition</title>
      <link>/project/csmt2019_pub/</link>
      <pubDate>Tue, 01 Sep 2020 23:47:26 +0800</pubDate>
      <guid>/project/csmt2019_pub/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Mar 2019 â€“ Jun 2019. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;Research Assistant for prof. CHEN Xiaoou in Wangxuan Institute of Computer Technology at Peking University.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Main Information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up a series of quartet database from DCMI database shared by China Conservatory of Music.&lt;/li&gt;
&lt;li&gt;Constructed an audio event detection model based on CRNN to detect and recognize instruments.&lt;/li&gt;
&lt;li&gt;Evaluated the percussion, recall rate and F-measure of the model and CNN baseline model, and compared the difference among different quartet databases generate from different music skills or music types.&lt;/li&gt;
&lt;li&gt;published &lt;a href=&#39;../../files/publication.pdf&#39; target=&#39;_blank&#39;&gt;result&lt;/a&gt; on Conference of Sound and Music Technology 2019.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>åŸºäºå·ç§¯å¾ªç¯ç¥ç»ç½‘çš„å¤éŸ³éŸ³ä¹ä¸­å›½æ°‘æ—ä¹å™¨æ£€æµ‹ (Detection of Chinese Instrumental Quartet based on CRNN)</title>
      <link>/publication/csmt2019/</link>
      <pubDate>Tue, 01 Sep 2020 23:22:00 +0800</pubDate>
      <guid>/publication/csmt2019/</guid>
      <description>&lt;p&gt;Participant of the conference.&lt;/p&gt;
&lt;p&gt;Published a paper entitled â€œåŸºäºå·ç§¯å¾ªç¯ç¥ç»ç½‘çš„å¤éŸ³éŸ³ä¹ä¸­å›½æ°‘æ—ä¹å™¨æ£€æµ‹â€(&lt;a href=&#39;../../files/publication.pdf&#39; target=&#39;_blank&#39;&gt;Detection of Chinese Instrumental Quartet based on CRNN.pdf&lt;/a&gt;).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
