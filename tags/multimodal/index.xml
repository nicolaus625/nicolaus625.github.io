<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multimodal | MIRer</title>
    <link>/tags/multimodal/</link>
      <atom:link href="/tags/multimodal/index.xml" rel="self" type="application/rss+xml" />
    <description>Multimodal</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 29 Sep 2023 21:40:17 +0100</lastBuildDate>
    <image>
      <url>/images/icon_hu25a3ac0bff28d67894e084ca1bc4b864_11852_512x512_fill_lanczos_center_2.png</url>
      <title>Multimodal</title>
      <link>/tags/multimodal/</link>
    </image>
    
    <item>
      <title>Bridging Music &amp; Text with Pre-trained Models for Music Captioning and QA</title>
      <link>/project/musilingo/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/musilingo/</guid>
      <description>&lt;p&gt;07/2023 â€“ present&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed Music Instruct (MI) query-response dataset based on captions &amp;amp; well-designed prompts to GPT-4.&lt;/li&gt;
&lt;li&gt;Achieved cutting-edge performance in question answering on both MusicQA and Music Instruct datasets.&lt;/li&gt;
&lt;li&gt;Employed instruct fine-tuning techniques on MI to attain state-of-the-art (SOTA) results in captioning.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response</title>
      <link>/publication/musilingo/</link>
      <pubDate>Tue, 19 Sep 2023 18:24:35 +0100</pubDate>
      <guid>/publication/musilingo/</guid>
      <description>&lt;p&gt;Abstract: Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.08730.pdf&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zihaod/musilingo&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/m-a-p/Music-Instruct/tree/main&#34; target=&#34;_blank&#34;&gt;Dataset link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To be submitted to ****2023.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
