<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MIRer</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>MIRer</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 24 Oct 2025 11:29:43 +0800</lastBuildDate>
    <image>
      <url>/images/icon_hu25a3ac0bff28d67894e084ca1bc4b864_11852_512x512_fill_lanczos_center_2.png</url>
      <title>MIRer</title>
      <link>/</link>
    </image>
    
    <item>
      <title>üéâüéâüéâAwarded Google PhD Fellowship 2025</title>
      <link>/publication/googlefellowship/</link>
      <pubDate>Fri, 24 Oct 2025 11:29:43 +0800</pubDate>
      <guid>/publication/googlefellowship/</guid>
      <description>&lt;p&gt;We are extremely proud to announce that Yinghao MA, a PhD student in AI and music at the Centre for Digital Music at QMUL, has been awarded the 2025 Google Fellowship in Machine Perception.&lt;/p&gt;
&lt;p&gt;The Google PhD Fellowship Program was created to recognise outstanding graduate students doing exceptional and innovative research in areas relevant to computer science and related fields.&lt;/p&gt;
&lt;p&gt;A Google spokesperson said: ‚ÄúThe student nominations we received this year were exemplary in their quality, but Yinghao especially stood out and was endorsed by the research scientists and distinguished engineers within Google who participated in the review. Congratulations to Yinghao on this well-deserved recognition, it‚Äôs an honor to support such incredibly talented students.‚Äù&lt;/p&gt;
&lt;p&gt;Yinghao Ma&#39;s PhD research focuses on advancing Large Language Models (LLMs) for music understanding and generation. Specifically, he studies how multimodal models can integrate audio, symbolic, and textual information to understand, reason about, and generate music.&lt;/p&gt;
&lt;p&gt;Together with colleagues, he developed MERT, a large-scale music audio representation model which has more than 10k monthly download in the past three years. His recent work includes developing music instruction-following datasets and benchmarks that help evaluate how well AI systems can comprehend and create music.&lt;/p&gt;
&lt;p&gt;He said: &amp;ldquo;It&#39;s my great honour to receive the Google PhD Fellowship that recognises my research and strongly contribute to my future career. I‚Äôm deeply grateful to Google and QMUL for the support, providing good platforms for AI &amp;amp; music research.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#39;https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-student-awarded-google-phd-fellowship.html&#39; target=&#39;_blank&#39;&gt;News @ QMUL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#39;https://www.linkedin.com/feed/update/urn:li:activity:7387160161896939521/&#39; target=&#39;_blank&#39;&gt;Linkedin post @ QMUL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#39;https://research.google/programs-and-events/phd-fellowship/recipients/&#39; target=&#39;_blank&#39;&gt;complete list of fellowship recipients on Google&#39;s website.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#39;https://blog.google/outreach-initiatives/google-org/phd-fellowship-program-2025/&#39; target=&#39;_blank&#39;&gt;News @ Google&#39;s blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>üéâAccepted by NeuriPS2025üéâ MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</title>
      <link>/publication/mmar/</link>
      <pubDate>Thu, 18 Sep 2025 18:24:35 +0100</pubDate>
      <guid>/publication/mmar/</guid>
      <description>&lt;p&gt;Abstract: We introduce MMAR, a new benchmark designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. MMAR comprises 1,000 meticulously curated audio-question-answer triplets, collected from real-world internet videos and refined through iterative error corrections and quality checks to ensure high quality. Unlike existing benchmarks that are limited to specific domains of sound, music, or speech, MMAR extends them to a broad spectrum of real-world audio scenarios, including mixed-modality combinations of sound, music, and speech. Each question in MMAR is hierarchically categorized across four reasoning layers: Signal, Perception, Semantic, and Cultural, with additional sub-categories within each layer to reflect task diversity and complexity. To further foster research in this area, we annotate every question with a Chain-of-Thought (CoT) rationale to promote future advancements in audio reasoning. Each item in the benchmark demands multi-step deep reasoning beyond surface-level understanding. Moreover, a part of the questions requires graduate-level perceptual and domain-specific knowledge, elevating the benchmark&#39;s difficulty and depth. We evaluate MMAR using a broad set of models, including Large Audio-Language Models (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models (OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with audio caption inputs. The performance of these models on MMAR highlights the benchmark&#39;s challenging nature, and our analysis further reveals critical limitations of ‚Ä¶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.13032&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ddlBoJack/MMAR&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the 39 Annual Conference on Neural Information Processing Systems (NeurIPS 2025).&lt;/li&gt;
&lt;li&gt;News of presentation on the &lt;a href=&#34;https://www.bmva.org/meetings/25-11-05-MultimodalLargeModels.html&#34; target=&#34;_blank&#34;&gt;British Machine Vision Association Workshop on Multimodal LLMs&lt;/a&gt;, please click - &lt;a href=&#34;https://www.c4dm.eecs.qmul.ac.uk/news/2025-11-05.BMVA-Workshop/&#34; target=&#34;_blank&#34;&gt;here.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>üéâAccepted by NeuriPS2025üéâ Omnibench: Towards the future of universal omni-language models</title>
      <link>/publication/omnibench/</link>
      <pubDate>Thu, 18 Sep 2025 18:24:35 +0100</pubDate>
      <guid>/publication/omnibench/</guid>
      <description>&lt;p&gt;Abstract: 
Recent advancements in multimodal large language models (MLLMs) have focused on integrating multiple modalities, yet their ability to simultaneously process and reason across different inputs remains underexplored. We introduce OmniBench, a novel benchmark designed to evaluate models&amp;rsquo; ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define language models capable of such tri-modal processing as omni-language models (OLMs). OmniBench features high-quality human annotations that require integrated understanding across all modalities. Our evaluation reveals that: i) open-source OLMs show significant limitations in instruction-following and reasoning in tri-modal contexts; and ii) most baseline models perform poorly (around 50% accuracy) even with textual alternatives to image/audio inputs. To address these limitations, we develop OmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We advocate for developing more robust tri-modal integration techniques and training strategies to enhance OLM performance. Codes and data could be found at our repo (&lt;a href=&#34;https://github.com/multimodal-art-projection/OmniBench)&#34;&gt;https://github.com/multimodal-art-projection/OmniBench)&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.15272&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/multimodal-art-projection/OmniBench&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the 39 Annual Conference on Neural Information Processing Systems (NeurIPS 2025).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following</title>
      <link>/publication/cmi-bench/</link>
      <pubDate>Sat, 14 Jun 2025 18:24:35 +0100</pubDate>
      <guid>/publication/cmi-bench/</guid>
      <description>&lt;p&gt;Abstract: Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking: reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2506.12285&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nicolaus625/CMI-bench&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/nicolaus625/CMI-bench&#34; target=&#34;_blank&#34;&gt;Dataset link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the 26th International Society for Music Information Retrieval (ISMIR 2025).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Yue: Scaling open foundation models for long-form music generation</title>
      <link>/publication/yue/</link>
      <pubDate>Tue, 11 Mar 2025 18:24:35 +0100</pubDate>
      <guid>/publication/yue/</guid>
      <description>&lt;p&gt;Abstract: We tackle the task of long-form music generation&amp;ndash;particularly the challenging \textbf{lyrics-to-song} problem&amp;ndash;by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE&#39;s learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Audio-flan: A preliminary release</title>
      <link>/publication/audio-flan/</link>
      <pubDate>Sun, 23 Feb 2025 18:24:35 +0100</pubDate>
      <guid>/publication/audio-flan/</guid>
      <description>&lt;p&gt;Abstract: 
Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supergpqa: Scaling llm evaluation across 285 graduate disciplines</title>
      <link>/publication/supergpqa/</link>
      <pubDate>Thu, 20 Feb 2025 18:24:35 +0100</pubDate>
      <guid>/publication/supergpqa/</guid>
      <description>&lt;p&gt;Abstract: 
Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Foundation models for music: A survey</title>
      <link>/publication/fm4music/</link>
      <pubDate>Mon, 26 Aug 2024 18:24:35 +0100</pubDate>
      <guid>/publication/fm4music/</guid>
      <description>&lt;p&gt;Abstract: In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music ‚Ä¶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nicolaus625/FM4Music&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Huggingface daily paper top 3, 27 Aug 2024.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Can LLMs &#34;Reason&#34; in Music? An Evaluation of LLMs&#39; Capability of Music Understanding and Generation</title>
      <link>/publication/reason/</link>
      <pubDate>Wed, 31 Jul 2024 18:24:35 +0100</pubDate>
      <guid>/publication/reason/</guid>
      <description>&lt;p&gt;Abstract: Symbolic Music, akin to language, can be encoded in discrete symbols. Recent research has extended the application of large language models (LLMs) such as GPT-4 and Llama2 to the symbolic music domain including understanding and generation. Yet scant research explores the details of how these LLMs perform on advanced music understanding and conditioned generation, especially from the multi-step reasoning perspective, which is a critical aspect in the conditioned, editable, and interactive human-computer co-creation process. This study conducts a thorough investigation of LLMs‚Äô capability and limitations in symbolic music processing. We identify that current LLMs exhibit poor performance in song-level multi-step music reasoning, and typically fail to leverage learned music knowledge when addressing complex musical tasks. An analysis of LLMs‚Äô responses highlights distinctly their pros and cons. Our findings suggest achieving advanced musical capability is not intrinsically obtained by LLMs, and future research should focus more on bridging the gap between music knowledge and reasoning, to improve the co-creation experience for musicians.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/html/2407.21531v1&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the 25th International Society for Music Information Retrieval (ISMIR 2024).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Domain-specific continual pre-training scaling law for large language models</title>
      <link>/publication/d-cpt/</link>
      <pubDate>Mon, 03 Jun 2024 18:24:35 +0100</pubDate>
      <guid>/publication/d-cpt/</guid>
      <description>&lt;p&gt;Abstract: Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely used to expand the model‚Äôs fundamental understanding of specific downstream domains (eg, math and code). For the CPT on domain-specific LLMs, one important question is how to choose the optimal mixture ratio between the general-corpus (eg, Dolma, Slim-pajama) and the downstream domain-corpus. Existing methods usually adopt laborious human efforts by grid-searching on a set of mixture ratios, which require high GPU training consumption costs. Besides, we cannot guarantee the selected ratio is optimal for the specific domain. To address the limitations of existing methods, inspired by the Scaling Law for performance prediction, we propose to investigate the Scaling Law of the Domain-specific Continual Pre-Training (D-CPT Law) to decide the optimal mixture ratio with acceptable training costs for LLMs of different sizes. Specifically, by fitting the D-CPT Law, we can easily predict the general and downstream performance of arbitrary mixture ratios, model sizes, and dataset sizes using small-scale training costs on limited experiments. Moreover, we also extend our standard D-CPT Law on cross-domain settings and propose the Cross-Domain D-CPT Law to predict the D-CPT law of target domains, where very small training costs (about 1% of the normal training costs) are needed for the target domains. Comprehensive experimental results on six downstream domains demonstrate the effectiveness and generalizability of our proposed D-CPT Law and Cross-Domain D-CPT Law.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/a4628e9fbd3002a554923642f74d5d6b-Paper-Conference.pdf&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Map-neo: Highly capable and transparent bilingual large language model series</title>
      <link>/publication/neo/</link>
      <pubDate>Wed, 29 May 2024 18:24:35 +0100</pubDate>
      <guid>/publication/neo/</guid>
      <description>&lt;p&gt;Abstract: Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model&#39;s weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our ‚Ä¶&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ComposerX: Multi-Agent Symbolic Music Composition with LLMs</title>
      <link>/publication/composorx/</link>
      <pubDate>Sun, 28 Apr 2024 18:24:35 +0100</pubDate>
      <guid>/publication/composorx/</guid>
      <description>&lt;p&gt;Abstract: Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. Current LLMs often struggle with this task, sometimes generating poorly written music even when equipped with modern techniques like InContext-Learning and Chain-of-Thoughts. To further explore and enhance LLMs‚Äô potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX 1 , an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/98627/Benetos%20ComposerX%3A%20Multi-Agent%20Symbolic%20Music%20Composition%20with%20LLMs%202024%20Published.pdf?sequence=2&amp;isAllowed=y&#34; arget=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the 25th International Society for Music Information Retrieval (ISMIR 2024).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Mupt: A generative symbolic music pretrained transformer</title>
      <link>/publication/mupt/</link>
      <pubDate>Tue, 09 Apr 2024 18:24:35 +0100</pubDate>
      <guid>/publication/mupt/</guid>
      <description>&lt;p&gt;Abstract: In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model&#39;s performance in musical composition. To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a Synchronized Multi-Track ABC Notation (SMT-ABC Notation), which aims to preserve coherence across multiple musical tracks. Our contributions include a series of models capable of handling up to 8192 tokens, covering 90% of the symbolic music data in our training set. Furthermore, we explore the implications of the Symbolic Music Scaling Law (SMS Law) on model performance. The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=iAK9oHp4Zz&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/multimodal-art-projection/MuPT&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in the Thirteenth International Conference on Learning Representations
(ICLR 2025).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chatmusician: Understanding and generating music intrinsically with LLM</title>
      <link>/publication/chatmusician/</link>
      <pubDate>Sun, 25 Feb 2024 18:24:35 +0100</pubDate>
      <guid>/publication/chatmusician/</guid>
      <description>&lt;p&gt;Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity&#39;s creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs can be an excellent compressor for music, but there remains significant territory to be conquered. We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cif-bench: A chinese instruction-following benchmark for evaluating the generalizability of large language models</title>
      <link>/publication/clf-bench/</link>
      <pubDate>Tue, 20 Feb 2024 18:24:35 +0100</pubDate>
      <guid>/publication/clf-bench/</guid>
      <description>&lt;p&gt;Abstract: The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate data contamination, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts. This work not only uncovers the current limitations of LLMs in handling Chinese language tasks but also sets a new standard for future LLM generalizability research, pushing towards the development of more adaptable, culturally informed, and linguistically diverse models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mertech: Instrument Playing Technique Detection Using Self-Supervised Pretrained Model with Multi-Task Finetuning</title>
      <link>/publication/mertech/</link>
      <pubDate>Sun, 15 Oct 2023 18:24:35 +0100</pubDate>
      <guid>/publication/mertech/</guid>
      <description>&lt;p&gt;Abstract: Instrument playing techniques (IPTs) constitute a pivotal component of musical expression. However, the development of automatic IPT detection methods suffers from limited labeled data and inherent class imbalance issues. In this paper, we propose to apply a self-supervised learning model pre-trained on large-scale unlabeled music data and finetune it on IPT detection tasks. This approach addresses data scarcity and class imbalance challenges. Recognizing the significance of pitch in capturing the nuances of IPTs and the importance of onset in locating IPT events, we investigate multi-task finetuning with pitch and onset detection as auxiliary tasks. Additionally, we apply a post-processing approach for event-level prediction, where an IPT activation initiates an event only if the onset output confirms an onset in that frame. Our method outperforms prior approaches in both frame-level and event-level metrics across multiple IPT benchmark datasets. Further experiments demonstrate the efficacy of multi-task finetuning on each IPT class. 1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10447445&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/LiDCC/MERTech&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2024).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bridging Music &amp; Text with Pre-trained Models for Music Captioning and QA</title>
      <link>/project/musilingo/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/musilingo/</guid>
      <description>&lt;p&gt;07/2023 ‚Äì present&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed Music Instruct (MI) query-response dataset based on captions &amp;amp; well-designed prompts to GPT-4.&lt;/li&gt;
&lt;li&gt;Achieved cutting-edge performance in question answering on both MusicQA and Music Instruct datasets.&lt;/li&gt;
&lt;li&gt;Employed instruct fine-tuning techniques on MI to attain state-of-the-art (SOTA) results in captioning.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MARBLE: Music Audio Representation Benchmark for Universal Evaluation</title>
      <link>/project/marble/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/marble/</guid>
      <description>&lt;p&gt;01/2023 ‚Äì 06/2023&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designing the downstream tasks, datasets, evaluation metrics and state-of-the-art results.&lt;/li&gt;
&lt;li&gt;Implementing the mir_eval metrics with torchmetrics and developing utilisation for sequential tasks.&lt;/li&gt;
&lt;li&gt;Establishing a fair, reproducible and universal music information retrieval benchmark for future work.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marble-bm.sheffield.ac.uk/&#34; target=&#34;_blank&#34;&gt;MARBLE website.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/project/mert/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/mert/</guid>
      <description>&lt;p&gt;08/2022 ‚Äì 05/2023&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Built self-supervised learning systems, acquiring 50k+ downloading of checkpoints on Huggingface.&lt;/li&gt;
&lt;li&gt;Replaced the pseudo-tag from MFCCs to Chroma music features for harmonic information.&lt;/li&gt;
&lt;li&gt;Utilising deep features like Encodec instead of k-means for scaling up models to 1 B parameters.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response</title>
      <link>/publication/musilingo/</link>
      <pubDate>Tue, 19 Sep 2023 18:24:35 +0100</pubDate>
      <guid>/publication/musilingo/</guid>
      <description>&lt;p&gt;Abstract: Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.08730.pdf&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zihaod/musilingo&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/m-a-p/Music-Instruct/tree/main&#34; target=&#34;_blank&#34;&gt;Dataset link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PUblished in 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2024).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MARBLE: Music Audio Representation Benchmark for Universal Evaluation</title>
      <link>/publication/marble/</link>
      <pubDate>Sun, 18 Jun 2023 18:24:35 +0100</pubDate>
      <guid>/publication/marble/</guid>
      <description>&lt;p&gt;Abstract: In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and reproducible suite for the community, with a clear statement on copyright issues on datasets. Results suggest recently proposed large-scale pre-trained musical language models perform the best in most tasks, with room for further improvement. The leaderboard and toolkit repository are published at this https URL to promote future music AI research.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.105488&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/a43992899/MARBLE-Benchmark&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Accepted by NeurIPS 2023, dataset and benchmark track.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/publication/mert/</link>
      <pubDate>Wed, 31 May 2023 18:24:35 +0100</pubDate>
      <guid>/publication/mert/</guid>
      <description>&lt;p&gt;Abstract: Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). These teachers effectively guide our student model, a BERT-style transformer encoder, to better model music audio. In addition, we introduce an in-batch noise mixture augmentation to enhance the representation robustness. Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attains state-of-the-art (SOTA) overall scores. The code ‚Ä¶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00107&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/yizhilll/MERT&#34; target=&#34;_blank&#34;&gt;Code link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rejected by NeurIPS2023&lt;/li&gt;
&lt;li&gt;Submitted to ICLR2024&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Time-Variant Reverberation Algorithm for Reverberation Enhancement Syetem</title>
      <link>/project/reverb/</link>
      <pubDate>Fri, 12 May 2023 22:05:06 -0400</pubDate>
      <guid>/project/reverb/</guid>
      <description>&lt;p&gt;The reverberation algorithm is usually an LTI system. The room in the concert hall does not
change, so the response does not change over time. However, this can lead to colouration and
instability due to feedback caused by the close proximity of the microphones and speakers. Some
time-varying systems can solve this problem. This paper implements a new time-varying variable
reverberation algorithm on bela, based on a combination of delay-line, lowpass filter and comb
filter and feedback, for reverberation enhancement systems. Such systems can often be used in
electroacoustically enhanced rehearsal rooms. This particular application is briefly outlined, and
other possible applications are discussed while the shortcomings of the experimental approach are
analysed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#39;../../files/map.mp4&#39; target=&#39;_blank&#39;&gt;Demo recoding&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On the effectiveness of speech self-supervised learning for musicCCF none</title>
      <link>/publication/ismir2023ssl/</link>
      <pubDate>Fri, 14 Apr 2023 18:24:35 +0100</pubDate>
      <guid>/publication/ismir2023ssl/</guid>
      <description>&lt;p&gt;Abstract: Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent speech models such as wav2vec2.0 have shown promise in music modelling. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train  SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.05161.pdf&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Accepted by ISMIR 2023, Milan, Italy.&lt;/li&gt;
&lt;li&gt;Music2Vec can be founded at here, the link is the same as our ISMIR2022LBD demo &lt;a href=&#34;https://huggingface.co/m-a-p/music2vec-v1&#34;&gt;https://huggingface.co/m-a-p/music2vec-v1&lt;/a&gt; and the training code is the same as the MERT training code. But the k-means feature need to be replaced with MFCC as the information described in the paper &lt;a href=&#34;https://github.com/yizhilll/MERT&#34;&gt;https://github.com/yizhilll/MERT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lyricwhiz: Robust multilingual zero-shot lyrics transcription by whispering to chatgptCCF none</title>
      <link>/publication/lyricwhiz/</link>
      <pubDate>Fri, 14 Apr 2023 17:24:35 +0100</pubDate>
      <guid>/publication/lyricwhiz/</guid>
      <description>&lt;p&gt;Abstract: We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today&#39;s most performant chat-based large language model. In the proposed method, Whisper functions as the &amp;ldquo;ear&amp;rdquo; by transcribing the audio, while GPT-4 serves as the &amp;ldquo;brain,&amp;rdquo; acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a human-annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.17103.pdf&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Accepted by ISMIR 2023.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Large-Scale Pretrained Model for Self-Supervised Music Audio Representation Learning</title>
      <link>/publication/dmrn2022/</link>
      <pubDate>Mon, 19 Dec 2022 18:24:35 +0100</pubDate>
      <guid>/publication/dmrn2022/</guid>
      <description>&lt;p&gt;Abstract: Self-supervised learning technique is an under-explored topic for music audio due to the challenge of designing an appropriate training paradigm. We hence propose MAP-MERT, a large-scale music audio pre-trained model for general music understanding. We achieve performance that is comparable to the state-of-the-art pre-trained model Jukebox using less than 2% of parameters.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://qmro.qmul.ac.uk/xmlui/handle/123456789/83372&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Presented by DMRN workshop 2022.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learnable Front Ends Based on Temporal Modulation for Music TaggingCCF none</title>
      <link>/publication/tmnn/</link>
      <pubDate>Mon, 05 Dec 2022 18:24:35 +0100</pubDate>
      <guid>/publication/tmnn/</guid>
      <description>&lt;p&gt;Abstract: While end-to-end systems are becoming popular in auditory signal processing including automatic music tagging, models using raw audio as input needs a large amount of data and computational resources without domain knowledge. Inspired by the fact that temporal modulation is regarded as an essential component in auditory perception, we introduce the Temporal Modulation Neural Network (TMNN) that combines Mel-like data-driven front ends and temporal modulation filters with a simple ResNet back end. The structure includes a set of temporal modulation filters to capture long-term patterns in all frequency channels. Experimental results show that the proposed front ends surpass state-of-the-art (SOTA) methods on the MagnaTagATune dataset in automatic music tagging, and they are also helpful for keyword spotting on speech commands. Moreover, the model performance for each tag suggests that genre or instrument tags with complex rhythm and mood tags can especially be improved with temporal modulation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.15254.pdf8&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accepted by ISMIR 2023 late breaking demo.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Map-music2vec: A simple and effective baseline for self-supervised music audio representation learning</title>
      <link>/publication/music2vec/</link>
      <pubDate>Mon, 05 Dec 2022 18:24:35 +0100</pubDate>
      <guid>/publication/music2vec/</guid>
      <description>&lt;p&gt;Abstract: The deep learning community has witnessed an exponentially growing interest in self-supervised learning (SSL). However, it still remains unexplored how to build a framework for learning useful representations of raw music waveforms in a self-supervised manner. In this work, we design Music2Vec, a framework exploring different SSL algorithmic components and tricks for music audio recordings. Our model achieves comparable results to the state-of-the-art (SOTA) music SSL model Jukebox, despite being significantly smaller with less than 2% of parameters of the latter. The model will be released on Huggingface.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.02508&#34; target=&#34;_blank&#34;&gt;Paper link.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/m-a-p/music2vec-v1&#34; target=&#34;_blank&#34;&gt;Huggingface checkpoints.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accepted by ISMIR 2022, late breaking demo.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/project/tmnn/</link>
      <pubDate>Mon, 29 Aug 2022 21:40:17 +0100</pubDate>
      <guid>/project/tmnn/</guid>
      <description>&lt;p&gt;09/2021 ‚Äì 07/2022&lt;/p&gt;
&lt;p&gt;Research Assistant, Supervised by Prof. Richard Stern, Carnegie Mellon University&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constructed 2-layer learnable front ends in Temporal Modulation Neural Network (TMNN) that combines
Mel-like data-driven front ends and temporal modulation filters.&lt;/li&gt;
&lt;li&gt;Examined the proposed front ends surpass state-of-the-art (SOTA) methods on the MagnaTagATune dataset in
automatic music tagging, and they are also helpful for keyword spotting on speech commands.&lt;/li&gt;
&lt;li&gt;Analysis of the model performance among tags with different genres and instrument tags.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Additional Notes in ICA especially for FOBI</title>
      <link>/post/fobi/</link>
      <pubDate>Mon, 15 Nov 2021 14:43:44 -0500</pubDate>
      <guid>/post/fobi/</guid>
      <description>&lt;p&gt;Suppose $A=(a_1, a_2,\cdots, a_N)^t$ is a $n$ dimension vectors, you can actually carry out the PCA $S = WA$ in the following ways:&lt;/p&gt;
&lt;p&gt;$E[SS^t]$ should be diagonal, since the components inside $S$ such as $s_1$ and $s_2$ should be uncorrelated, that is $E[s_1s_2]=0=E[s_1]E[s_2]$. &lt;strong&gt;(Here, we suppose that $A$ has zero mean, as well as $S=WA$)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The point is that, the uncorrelation is simply second order moment decomposition, which is not enough to make good use of the independence assumption (actually far from independence). Recall that the independent variables are uncorrelated but uncorrelated R.V. are generally not independent, except for the case the two R.V. are both follow binary distribution or joint Gaussian distribution. We should find something more, either higher order tensor decomposition, or use neg-entropy as a virtue / specific excellence / indicator of independence.&lt;/p&gt;
&lt;p&gt;if you choose some function other than higher order moment (such as Kurtosis or neg-entropy), you will get some kind of Fast-ICA algorithm in, which I will introduce to you guys in the following lecture. But if you choose to use the higher order tensor decomposition, you can still get a bunch of algorithm depends on which moment or the function of moment you choose to use. If you use the third moment, it will be harder to evaluate compared to fourth order, but you can still go to the &lt;strong&gt;Robust FOBI&lt;/strong&gt; algorithm given by Vardoso. If you use the fourth moment, it will give you &lt;strong&gt;FOBI&lt;/strong&gt; algorithm which is one of the most commonly used ICA implement via tensor decomposition. You can also use something looks weird to get &lt;strong&gt;Joint Approximation Diagonalization of Eigen- matrices (JADE)&lt;/strong&gt;, you can find a short introduction in hidden slide.&lt;/p&gt;
&lt;p&gt;Back to the FOBI algorithm, we will evaluate $D_A:=E[A^tAAA^t]$ for any given random vector $A=(a_1, a_2,\cdots, a_N)^t$ and all the entry of matrix $D_A$ will represent some kind of fourh order moment, like the following cases Bhiksha display in class&lt;/p&gt;
&lt;p&gt;$S=(s_1, s_2)^t$, then $D_S$ is a 2 by 2 matrix as follow: $$D_S = \begin{bmatrix} (s_1^2+s_2^2)s_1^2 &amp;amp; (s_1^2+s_2^2)s_1s_2 \ (s_1^2+s_2^2)s_2s_1 &amp;amp; (s_1^2+s_2^2)s_2^2 \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;Ingeneral, $D_A$ will be diagonal if the components of $A$ (that is, $a_1$, $a_2$, &amp;hellip;, $a_N$ are pairwisely independent). ALthough not vise versa, we can get some kind of &amp;ldquo;pseudo independence&amp;rdquo; if we can diagonalize $D_A$. So what FOBI-ICA does is actually find some useful $W$ (and turn $X$ to $\hat{X}$ at the same time actually), to get a diagonalized $D_S$ correspondent to the (pseudo) independent source $S$ from the indicator matrix $D_X$ from your observation $X$.&lt;/p&gt;
&lt;p&gt;But, recall that $S=W_0X$, the problem is that if you attempt to calculate $D_S=E[(X^tW_0^t)(W_0X)(W_0X)(X^tW_0^t)]$, you will feel Frustrated and get cold feet. So the strategy is to whiten the data by the following steps (forget the intuition I&#39;m trying to convey in the slides why people came out the fancy idea that we can actually whiten the data):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$X:=X-\mu_x$ to make X zero mean&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;evaluate the eighen decomposition (also singular value decomposition) of $E[XX^t]=P\Lambda P^t$, where $\Lambda$ is a diagnal matrix and $P$ is a unitary matrix&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;whiten the data $\hat{X}=\Lambda^{-0.5}P^t$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this way, you can prove that $e[\hat{X}\hat{X}^t] = I$ and the transformation between $\hat{X}$ and $S$ is a unitary matrix. (I proved them in class, hopfully they did not scare you.)&lt;/p&gt;
&lt;p&gt;And with these 2 good properties listed above, we can come to the following equation from $D_S=E[(\hat{X}^tW^t)(W\hat{X})(W\hat{X})(\hat{X}^tW^t)]$:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/fobi1.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;this means that if you would like to diagonalize matrix $D_S$ with parameter $W$, the only thing you need to do is to whiten the data from $X$ to $\hat{X}$ and get the singular value decomposition / eigen value decomposition of indecator matrix (consist of the forth order moment) of $\hat{X}$. From the decomposition, you can get the transformation $W$ from $X$ to $S$.&lt;/p&gt;
&lt;p&gt;To sum up, you simply get the following procedure&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/fobi2.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;P.S. some remarks for FOBI&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(0) Someone read the lecture notes on ICA in previous year and ask the relation between FOBI and Kurtosis / Neg-entropy and Fast-ICA. FOBI use tensor decomposition to implement the (pseudo) independent while Fast-ICA use the measure of Gaussian, Kurtosis and Neg-entropy are two kind of them. You can also try the one with mutual information function although we did not mentioned this function in the information theory lecture last week.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(1) FOBI is one of the first and most simple ICA methods and you can easily implement it on your own as ski-learn dows not implement it for you I believe.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(2) Whiten data can reduce the freedom dimension of $W$ when you tring to diagonalize $D_S=E[(X^tW_0^t)(W_0X)(W_0X)(X^tW_0^t)]$ and fasten the convergence. To be specific, the dimension of $W_0$ is $N^2$ or $N^2-1$ if condition on $\left| W_0 \right|=1$, while the dimension of unitary matrix $W$ is only $\frac{N^2-N}{2}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(3) The most notable drawback of FOBI require all the sources have quite distant in their fourth order moment values, implicating the failure in case of having several mechanisms characterized with the same distribution&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(4) FastICA based on Gaussian measure generally performances better compared to FOBI or something like that in case of high-dimensional data. So industry often chooses Fast-ICA for linear cases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(5) Fast-ICA based on Kurtosis is not robust enough as you can not get exact Kurtosis from less sample / observation of the variable, although it is easier to implement. While Fast-ICA based on Neg-entropy is not use Neg-entropy itself actually, as we all see that you can hardly evaluate Neg-entropy from even 1 million sample of observation. The approach is to find some approximation to Neg-entropy which can both be a robust measure to Gaussian, and not that hard to evaluate from the samples / observation. The latter should be the most commonly used approaches on Fast-ICA&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hopefully, this additional note can help you guys clear the following question: (1) why fourth order? (2) where is the tensor &lt;strong&gt;decomposition&lt;/strong&gt; happens? (3) why we need to whiten the data (4) why $W$ is unitary matrix (5) relation between fobi and fast-ica and other methods (6) what is the connection between ICA and PCA &amp;hellip;&lt;/p&gt;
&lt;p&gt;If you still have some questions about it, you can refer to the 
&lt;a href=&#34;https://mlsp2021.cs.cmu.edu/ppt/Lecture%20ICA%20new2.pdf&#34; target=&#34;_blank&#34;&gt;lecture notes&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learnable Frontend for Music, Speech and Audiow</title>
      <link>/project/masterthesis/</link>
      <pubDate>Tue, 02 Nov 2021 22:05:06 -0400</pubDate>
      <guid>/project/masterthesis/</guid>
      <description>&lt;p&gt;Design learnable frontends for deep learning models inspired by classic filters, multi-rate sampling &amp;amp; modulation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cover Song Detection &amp; Evaluation of Automatic Speech Recognition</title>
      <link>/project/teencent2021/</link>
      <pubDate>Tue, 02 Nov 2021 21:49:27 -0400</pubDate>
      <guid>/project/teencent2021/</guid>
      <description>&lt;p&gt;May 2020 &amp;ndash; Aug. 2021. Beijing, CHN. Summer internship in Tencent Holdings Limited. (Beijing)&lt;/p&gt;
&lt;p&gt;Write literature review on coversong detection.&lt;/p&gt;
&lt;p&gt;Reproduced and refined music separation &amp;amp; covering detection.&lt;/p&gt;
&lt;p&gt;Evaluating different ASR models on business data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Link to Chinese Blogs on Zhihu (Áü•‰πé)</title>
      <link>/post/link/</link>
      <pubDate>Tue, 02 Nov 2021 21:33:32 -0400</pubDate>
      <guid>/post/link/</guid>
      <description>&lt;p&gt;The 
&lt;a href=&#34;https://www.zhihu.com/column/c_1277025635056648192&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;
to zhihu column.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Âü∫‰∫éÊú∫Âô®Â≠¶‰π†ÁöÑÁ¨õÂ≠êÊºîÂ•èÊäÄÊ≥ï ËØÜÂà´Á†îÁ©∂‰∏éÂÆûÁé∞ (Research &amp; implementation of Chinese flute playing technique recognition based on ML)</title>
      <link>/publication/graduation/</link>
      <pubDate>Wed, 02 Sep 2020 11:29:43 +0800</pubDate>
      <guid>/publication/graduation/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Feb. 2020 ‚Äì May 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;One of the graduation theses that awarded the outstanding &lt;a href=&#39;../../files/ÊØïËÆæËÆ∫Êñá.pdf&#39; target=&#39;_blank&#39;&gt;paper&lt;/a&gt; honor of School of Mathematical Science, Peking University.&lt;/li&gt;
&lt;li&gt;Supervised by prof. CHEN Xiaoou in Wangxuan Institute of Computer Technology, PKU.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;see at project for more information.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tempo Detection of Chinese Pop Music</title>
      <link>/project/summer_deepmusic_mir/</link>
      <pubDate>Wed, 02 Sep 2020 00:21:40 +0800</pubDate>
      <guid>/project/summer_deepmusic_mir/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Jun. 2020 &amp;ndash; Sept. 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;Summer internship in Beijing Deepmusic Technology Co. LTD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Write literature review on song tempo/speed detection.&lt;/p&gt;
&lt;p&gt;Designing new model on tempo detection based on BiLSTM and Temporal Convolution Network(TCN) and compared them with the baselines of Librosa and MadMOM using the data provided by Renren Karaoke Company (more than 2000 songs manually marked by my colleagues).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the music with stable speed or with or a slightly slower ending, the accuracy of tempo recognition is above 87% without considering the double frequency (error is less than or equal to 0.01) and above 98% general accuracy (error is less than 0.1). While MadMOM less than 90%.&lt;/li&gt;
&lt;li&gt;The accuracy in distinguishing between three beats(three four beats, six eight beats, twelvw eight beats etc.) and four beats(four four beats etc.) is 95 percent.&lt;/li&gt;
&lt;li&gt;The local tempo tasks such as song&#39;s inherent shift and whether the ending slows down deserve more attention&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Research &amp; implementation of Chinese flute playing technique recognition based on machine learning</title>
      <link>/project/graduate/</link>
      <pubDate>Wed, 02 Sep 2020 00:21:10 +0800</pubDate>
      <guid>/project/graduate/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Feb. 2020 ‚Äì May 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;One of the graduation theses that awarded the outstanding &lt;a href=&#39;../../files/ÊØïËÆæËÆ∫Êñá.pdf&#39; target=&#39;_blank&#39;&gt;paper&lt;/a&gt; honor of School of Mathematical Science, Peking University.&lt;/li&gt;
&lt;li&gt;Research Assistant for prof. CHEN Xiaoou in Wangxuan Institute of Computer Technology at Peking University.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Music object recognition and recording is the essential component of music information retrieval. Different from other fields of melody extraction and music transcription, the research on musical instrument technique detection is still in the early stage. The existing work mainly focuses on technique detection of individual notes or frame, and there is a lack of effective data set.&lt;/p&gt;
&lt;p&gt;This article constructed &lt;a href=&#39;https://pan.baidu.com/s/1Czf6ZYqkW1EEpZeJh_3_gw&#39; target=&#39;_blank&#39;&gt;audio dataset&lt;/a&gt;(extraction code &amp;ldquo;jvbk&amp;rdquo; if needed) on ten kinds of techniques of transverse Chinese bamboo flute (Di), with 101 minutes of audio, using Mel frequency spectrum as audio feature, and put forward a model based on the fully convolutional neural network (FCNN). This model is an end-to-end sound event detector for variable length of the input and can be used in the detection of
instruments technology.&lt;/p&gt;
&lt;p&gt;Compared to the model based on VGG13, VGG16 and LeNet-5 baseline to evaluate the effectiveness of the proposed framework, this model got the average accuracy 94%, much better than all the others, on input of the different length of audio, which suggest good generalization ability.&lt;/p&gt;
&lt;p&gt;Key WordsÔºö sound event detection, music information retrieval, fully convolutional neural network, musical instrumet technique detection&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TONE CONTOUR REALIZATION IN SUNG CANTONESE</title>
      <link>/project/phonetics/</link>
      <pubDate>Wed, 02 Sep 2020 00:19:18 +0800</pubDate>
      <guid>/project/phonetics/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Nov. 2019 &amp;ndash; Jan. 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Class project&lt;/em&gt;, scored 92/100. Supervised by Associate prof. &lt;a href=&#39;https://chinese.pku.edu.cn/rwfc/1224333.htm&#39; target=&#39;_blank&#39;&gt;WANG Yunjia&lt;/a&gt;, department of Chinese Language and Literature, Peking University.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Final project of &lt;em&gt;Fundamental Chinese Phonetics&lt;/em&gt; course: A brief discussion on the relationship between the basic frequency and the lyric tone of Cantonese songs (class final paper see at &lt;a href=&#39;../../files/Áï•ËÆ∫Á≤§ËØ≠Ê≠åÊõ≤Âü∫È¢ë‰∏éÊ≠åËØçÂ£∞Ë∞ÉÁöÑÂÖ≥ËÅî.pdf&#39; target=&#39;_blank&#39;&gt;Áï•ËÆ∫Á≤§ËØ≠Ê≠åÊõ≤Âü∫È¢ë‰∏éÊ≠åËØçÂ£∞Ë∞ÉÁöÑÂÖ≥ËÅî&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Based on &lt;a href=&#39;https://pan.baidu.com/s/1hjZrIAGYvPX7zviB87aeFA&#39; target=&#39;_blank&#39;&gt;acoustic phonetics data&lt;/a&gt;(extraction code &amp;ldquo;uwr8&amp;rdquo; if needed), this paper discusses the influence of the fundamental frequency of cantonese songs on the tone of the lyrics. Eleven phonators (6 female and 5 male) aged between 18 and 22 years old were analyzed for the singing and lyric reading sound of the designated song, to explore the change of fundamental frequency over time, and the study tone involved four of the six tones in Cantonese.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;paires t-test&lt;/em&gt; of the acoustic data showed that :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there was a significant difference in the slope of the fundamental frequency of lyric reading over time in tones with different tone values;&lt;/li&gt;
&lt;li&gt;There is no significant difference in the slope of the fundamental frequency of the song over time between the lyrics of each tone;&lt;/li&gt;
&lt;li&gt;There is a certain relationship between the basic frequency slope of each tone and the basic frequency slope of the song.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chinese instrument recognition</title>
      <link>/project/csmt2019_pub/</link>
      <pubDate>Tue, 01 Sep 2020 23:47:26 +0800</pubDate>
      <guid>/project/csmt2019_pub/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Mar 2019 ‚Äì Jun 2019. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;Research Assistant for prof. CHEN Xiaoou in Wangxuan Institute of Computer Technology at Peking University.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Main Information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up a series of quartet database from DCMI database shared by China Conservatory of Music.&lt;/li&gt;
&lt;li&gt;Constructed an audio event detection model based on CRNN to detect and recognize instruments.&lt;/li&gt;
&lt;li&gt;Evaluated the percussion, recall rate and F-measure of the model and CNN baseline model, and compared the difference among different quartet databases generate from different music skills or music types.&lt;/li&gt;
&lt;li&gt;published &lt;a href=&#39;../../files/publication.pdf&#39; target=&#39;_blank&#39;&gt;result&lt;/a&gt; on Conference of Sound and Music Technology 2019.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Âü∫‰∫éÂç∑ÁßØÂæ™ÁéØÁ•ûÁªèÁΩëÁöÑÂ§çÈü≥Èü≥‰πê‰∏≠ÂõΩÊ∞ëÊóè‰πêÂô®Ê£ÄÊµã (Detection of Chinese Instrumental Quartet based on CRNN)</title>
      <link>/publication/csmt2019/</link>
      <pubDate>Tue, 01 Sep 2020 23:22:00 +0800</pubDate>
      <guid>/publication/csmt2019/</guid>
      <description>&lt;p&gt;Participant of the conference.&lt;/p&gt;
&lt;p&gt;Published a paper entitled ‚ÄúÂü∫‰∫éÂç∑ÁßØÂæ™ÁéØÁ•ûÁªèÁΩëÁöÑÂ§çÈü≥Èü≥‰πê‰∏≠ÂõΩÊ∞ëÊóè‰πêÂô®Ê£ÄÊµã‚Äù(&lt;a href=&#39;../../files/publication.pdf&#39; target=&#39;_blank&#39;&gt;Detection of Chinese Instrumental Quartet based on CRNN.pdf&lt;/a&gt;).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning How to Write Blog</title>
      <link>/post/name_post/</link>
      <pubDate>Tue, 01 Sep 2020 22:48:26 +0800</pubDate>
      <guid>/post/name_post/</guid>
      <description>&lt;p&gt;main part of the blog with markdown gramma&lt;/p&gt;
&lt;h1 id=&#34;title&#34;&gt;title&lt;/h1&gt;
&lt;h2 id=&#34;second-title&#34;&gt;second title&lt;/h2&gt;
&lt;p&gt;common words&lt;/p&gt;
&lt;p&gt;use the following codes in the powershell to add a pages document for post(blog), recent talk or recent publication:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;hugo new  --kind post post/ my-article-name 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;than there will be a new .md doc in your file &amp;ldquo;./content/post/&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Correspondence between Speech Melody and Pitch Contour in Sichuan Folk Song</title>
      <link>/project/duan_zhiyao/</link>
      <pubDate>Mon, 02 Sep 2019 00:19:03 +0800</pubDate>
      <guid>/project/duan_zhiyao/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Jul. 2019 ‚Äì Sept. 2019. Rochester, NY, US.&lt;/li&gt;
&lt;li&gt;Research Assistant supervised by &lt;a href=&#39;http://www2.ece.rochester.edu/~zduan/&#39; target=&#39;_blank&#39;&gt;prof. DUAN Zhiyao&lt;/a&gt;, Deparment of Electronic Computer Engineer, University of Rochester.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Main Information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Made a literature review of linguistics papers on relations between speech melody and notes in tone language song.&lt;/li&gt;
&lt;li&gt;Set up a database on Sichuan folk song with music scores in MusicXML form, lyrics audio in Sichuan dialect in wav form and note-audio alignment character by character.&lt;/li&gt;
&lt;li&gt;Analyzed the correspondence among the tone of each character, the change of fundamental frequency of each character audio and the change of music notes linked to it.&lt;/li&gt;
&lt;li&gt;Evaluated different correspondence rates in different type of folk songs&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
