<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | MIRer</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 15 Nov 2021 14:43:44 -0500</lastBuildDate>
    <image>
      <url>/images/icon_hu25a3ac0bff28d67894e084ca1bc4b864_11852_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Additional Notes in ICA especially for FOBI</title>
      <link>/post/fobi/</link>
      <pubDate>Mon, 15 Nov 2021 14:43:44 -0500</pubDate>
      <guid>/post/fobi/</guid>
      <description>&lt;p&gt;Suppose $A=(a_1, a_2,\cdots, a_N)^t$ is a $n$ dimension vectors, you can actually carry out the PCA $S = WA$ in the following ways:&lt;/p&gt;
&lt;p&gt;$E[SS^t]$ should be diagonal, since the components inside $S$ such as $s_1$ and $s_2$ should be uncorrelated, that is $E[s_1s_2]=0=E[s_1]E[s_2]$. &lt;strong&gt;(Here, we suppose that $A$ has zero mean, as well as $S=WA$)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The point is that, the uncorrelation is simply second order moment decomposition, which is not enough to make good use of the independence assumption (actually far from independence). Recall that the independent variables are uncorrelated but uncorrelated R.V. are generally not independent, except for the case the two R.V. are both follow binary distribution or joint Gaussian distribution. We should find something more, either higher order tensor decomposition, or use neg-entropy as a virtue / specific excellence / indicator of independence.&lt;/p&gt;
&lt;p&gt;if you choose some function other than higher order moment (such as Kurtosis or neg-entropy), you will get some kind of Fast-ICA algorithm in, which I will introduce to you guys in the following lecture. But if you choose to use the higher order tensor decomposition, you can still get a bunch of algorithm depends on which moment or the function of moment you choose to use. If you use the third moment, it will be harder to evaluate compared to fourth order, but you can still go to the &lt;strong&gt;Robust FOBI&lt;/strong&gt; algorithm given by Vardoso. If you use the fourth moment, it will give you &lt;strong&gt;FOBI&lt;/strong&gt; algorithm which is one of the most commonly used ICA implement via tensor decomposition. You can also use something looks weird to get &lt;strong&gt;Joint Approximation Diagonalization of Eigen- matrices (JADE)&lt;/strong&gt;, you can find a short introduction in hidden slide.&lt;/p&gt;
&lt;p&gt;Back to the FOBI algorithm, we will evaluate $D_A:=E[A^tAAA^t]$ for any given random vector $A=(a_1, a_2,\cdots, a_N)^t$ and all the entry of matrix $D_A$ will represent some kind of fourh order moment, like the following cases Bhiksha display in class&lt;/p&gt;
&lt;p&gt;$S=(s_1, s_2)^t$, then $D_S$ is a 2 by 2 matrix as follow: $$D_S = \begin{bmatrix} (s_1^2+s_2^2)s_1^2 &amp;amp; (s_1^2+s_2^2)s_1s_2 \ (s_1^2+s_2^2)s_2s_1 &amp;amp; (s_1^2+s_2^2)s_2^2 \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;Ingeneral, $D_A$ will be diagonal if the components of $A$ (that is, $a_1$, $a_2$, &amp;hellip;, $a_N$ are pairwisely independent). ALthough not vise versa, we can get some kind of &amp;ldquo;pseudo independence&amp;rdquo; if we can diagonalize $D_A$. So what FOBI-ICA does is actually find some useful $W$ (and turn $X$ to $\hat{X}$ at the same time actually), to get a diagonalized $D_S$ correspondent to the (pseudo) independent source $S$ from the indicator matrix $D_X$ from your observation $X$.&lt;/p&gt;
&lt;p&gt;But, recall that $S=W_0X$, the problem is that if you attempt to calculate $D_S=E[(X^tW_0^t)(W_0X)(W_0X)(X^tW_0^t)]$, you will feel Frustrated and get cold feet. So the strategy is to whiten the data by the following steps (forget the intuition I&#39;m trying to convey in the slides why people came out the fancy idea that we can actually whiten the data):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$X:=X-\mu_x$ to make X zero mean&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;evaluate the eighen decomposition (also singular value decomposition) of $E[XX^t]=P\Lambda P^t$, where $\Lambda$ is a diagnal matrix and $P$ is a unitary matrix&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;whiten the data $\hat{X}=\Lambda^{-0.5}P^t$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this way, you can prove that $e[\hat{X}\hat{X}^t] = I$ and the transformation between $\hat{X}$ and $S$ is a unitary matrix. (I proved them in class, hopfully they did not scare you.)&lt;/p&gt;
&lt;p&gt;And with these 2 good properties listed above, we can come to the following equation from $D_S=E[(\hat{X}^tW^t)(W\hat{X})(W\hat{X})(\hat{X}^tW^t)]$:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/fobi1.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;this means that if you would like to diagonalize matrix $D_S$ with parameter $W$, the only thing you need to do is to whiten the data from $X$ to $\hat{X}$ and get the singular value decomposition / eigen value decomposition of indecator matrix (consist of the forth order moment) of $\hat{X}$. From the decomposition, you can get the transformation $W$ from $X$ to $S$.&lt;/p&gt;
&lt;p&gt;To sum up, you simply get the following procedure&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/fobi2.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;P.S. some remarks for FOBI&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(0) Someone read the lecture notes on ICA in previous year and ask the relation between FOBI and Kurtosis / Neg-entropy and Fast-ICA. FOBI use tensor decomposition to implement the (pseudo) independent while Fast-ICA use the measure of Gaussian, Kurtosis and Neg-entropy are two kind of them. You can also try the one with mutual information function although we did not mentioned this function in the information theory lecture last week.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(1) FOBI is one of the first and most simple ICA methods and you can easily implement it on your own as ski-learn dows not implement it for you I believe.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(2) Whiten data can reduce the freedom dimension of $W$ when you tring to diagonalize $D_S=E[(X^tW_0^t)(W_0X)(W_0X)(X^tW_0^t)]$ and fasten the convergence. To be specific, the dimension of $W_0$ is $N^2$ or $N^2-1$ if condition on $\left| W_0 \right|=1$, while the dimension of unitary matrix $W$ is only $\frac{N^2-N}{2}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(3) The most notable drawback of FOBI require all the sources have quite distant in their fourth order moment values, implicating the failure in case of having several mechanisms characterized with the same distribution&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(4) FastICA based on Gaussian measure generally performances better compared to FOBI or something like that in case of high-dimensional data. So industry often chooses Fast-ICA for linear cases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(5) Fast-ICA based on Kurtosis is not robust enough as you can not get exact Kurtosis from less sample / observation of the variable, although it is easier to implement. While Fast-ICA based on Neg-entropy is not use Neg-entropy itself actually, as we all see that you can hardly evaluate Neg-entropy from even 1 million sample of observation. The approach is to find some approximation to Neg-entropy which can both be a robust measure to Gaussian, and not that hard to evaluate from the samples / observation. The latter should be the most commonly used approaches on Fast-ICA&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hopefully, this additional note can help you guys clear the following question: (1) why fourth order? (2) where is the tensor &lt;strong&gt;decomposition&lt;/strong&gt; happens? (3) why we need to whiten the data (4) why $W$ is unitary matrix (5) relation between fobi and fast-ica and other methods (6) what is the connection between ICA and PCA &amp;hellip;&lt;/p&gt;
&lt;p&gt;If you still have some questions about it, you can refer to the 
&lt;a href=&#34;https://mlsp2021.cs.cmu.edu/ppt/Lecture%20ICA%20new2.pdf&#34; target=&#34;_blank&#34;&gt;lecture notes&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Link to Chinese Blogs on Zhihu (知乎)</title>
      <link>/post/link/</link>
      <pubDate>Tue, 02 Nov 2021 21:33:32 -0400</pubDate>
      <guid>/post/link/</guid>
      <description>&lt;p&gt;The 
&lt;a href=&#34;https://www.zhihu.com/column/c_1277025635056648192&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;
to zhihu column.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning How to Write Blog</title>
      <link>/post/name_post/</link>
      <pubDate>Tue, 01 Sep 2020 22:48:26 +0800</pubDate>
      <guid>/post/name_post/</guid>
      <description>&lt;p&gt;main part of the blog with markdown gramma&lt;/p&gt;
&lt;h1 id=&#34;title&#34;&gt;title&lt;/h1&gt;
&lt;h2 id=&#34;second-title&#34;&gt;second title&lt;/h2&gt;
&lt;p&gt;common words&lt;/p&gt;
&lt;p&gt;use the following codes in the powershell to add a pages document for post(blog), recent talk or recent publication:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;hugo new  --kind post post/ my-article-name 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;than there will be a new .md doc in your file &amp;ldquo;./content/post/&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
