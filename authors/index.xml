<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Authors | MIRer</title>
    <link>/authors/</link>
      <atom:link href="/authors/index.xml" rel="self" type="application/rss+xml" />
    <description>Authors</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>/images/icon_hu25a3ac0bff28d67894e084ca1bc4b864_11852_512x512_fill_lanczos_center_2.png</url>
      <title>Authors</title>
      <link>/authors/</link>
    </image>
    
    <item>
      <title></title>
      <link>/authors/admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/authors/admin/</guid>
      <description>&lt;p&gt;MA Yinghao (马英浩) is a Ph.D. candidate in Artificial Intelligence and Music (&lt;a href=&#34;https://www.aim.qmul.ac.uk/&#34; target=&#34;_blank&#34;&gt;AIM&lt;/a&gt;) program at Centre for Digital Music (&lt;a href=&#34;https://c4dm.eecs.qmul.ac.uk/&#34; target=&#34;_blank&#34;&gt;C4DM&lt;/a&gt;), School of EECS, Queen Mary University of London, supervised by Dr. Emmanouil Benetos, Dr. Chris Donahue (secondary), and Prof. Simon Dixon (independent assessor). &lt;!-- His research interests include machine learning (especially self-supervised learning, SSL) and signal processing for music information retrieval (MIR) and  multimodal learning. --&gt; 
He is one of the co-founders of the Multimodal Art Projection (&lt;a href=&#34;https://m-a-p.ai/&#34; target=&#34;_blank&#34;&gt;MAP&lt;/a&gt;) community.
Together with his colleague, he proposed an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), with more than 10k monthly downloads on the &lt;a href=&#34;https://huggingface.co/m-a-p/&#34; target=&#34;_blank&#34;&gt;Huggingfac page&lt;/a&gt;, established a Music Audio Representation Benchmark for universaL Evaluation (&lt;a href=&#34;https://marble-bm.sheffield.ac.uk/&#34; target=&#34;_blank&#34;&gt;MRABLE&lt;/a&gt;), and developped music generation GPT models such as &lt;a href=&#34;https://openreview.net/forum?id=iAK9oHp4Zz&amp;referrer=%5BAuthor+Console%5D%28%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions%29&#34; target=&#34;_blank&#34;&gt;MuPT&lt;/a&gt;.
He is also interested in music-related multimodality and developed &lt;a href=&#34;https://github.com/zihaod/MusiLingo&#34; target=&#34;_blank&#34;&gt;MusiLingo&lt;/a&gt;, a music captioning and query response model based on the alignment of single-modality pre-trained models along with multimodal reasoning benchmark including &lt;a href=&#34;https://github.com/multimodal-art-projection/OmniBench&#34; target=&#34;_blank&#34;&gt;OmniBench&lt;/a&gt; and &lt;a href=&#34;https://github.com/ddlBoJack/MMAR&#34; target=&#34;_blank&#34;&gt;MMAR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Besides, he was one of student conductors of Chinese Philharmonic Orchestra, 
Chinese Music Institute at Peking University (&lt;a href=&#34;https://www.facebook.com/Peking-University-Chinese-Music-Institute-cmipku-709151079161865/?ref=bookmarks&#34; target=&#34;_blank&#34;&gt;Facebook Page&lt;/a&gt;). 
He is also an advocate of charitable activities (see at &lt;a href=&#34;#experience&#34;&gt;other experience&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;He is going to be open to full-time position at autumn 2026 on foundation model for music-related multimodality. &lt;!-- in order to pave the way for a human understanding of music phenomenon. He is now applying for Graduate studies, in pursuit of his passion in exploring new things and innovating new ideas. --&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
