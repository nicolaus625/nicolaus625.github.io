<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | MIRer</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 29 Sep 2023 21:40:17 +0100</lastBuildDate>
    <image>
      <url>/images/icon_hu25a3ac0bff28d67894e084ca1bc4b864_11852_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>Bridging Music &amp; Text with Pre-trained Models for Music Captioning and QA</title>
      <link>/project/musilingo/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/musilingo/</guid>
      <description>&lt;p&gt;07/2023 – present&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed Music Instruct (MI) query-response dataset based on captions &amp;amp; well-designed prompts to GPT-4.&lt;/li&gt;
&lt;li&gt;Achieved cutting-edge performance in question answering on both MusicQA and Music Instruct datasets.&lt;/li&gt;
&lt;li&gt;Employed instruct fine-tuning techniques on MI to attain state-of-the-art (SOTA) results in captioning.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MARBLE: Music Audio Representation Benchmark for Universal Evaluation</title>
      <link>/project/marble/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/marble/</guid>
      <description>&lt;p&gt;01/2023 – 06/2023&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designing the downstream tasks, datasets, evaluation metrics and state-of-the-art results.&lt;/li&gt;
&lt;li&gt;Implementing the mir_eval metrics with torchmetrics and developing utilisation for sequential tasks.&lt;/li&gt;
&lt;li&gt;Establishing a fair, reproducible and universal music information retrieval benchmark for future work.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marble-bm.sheffield.ac.uk/&#34; target=&#34;_blank&#34;&gt;MARBLE website.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/project/mert/</link>
      <pubDate>Fri, 29 Sep 2023 21:40:17 +0100</pubDate>
      <guid>/project/mert/</guid>
      <description>&lt;p&gt;08/2022 – 05/2023&lt;/p&gt;
&lt;p&gt;Supervised by Dr Emmanouil Benetos, Centre for Digital Music, Queen Mary University of London&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Built self-supervised learning systems, acquiring 50k+ downloading of checkpoints on Huggingface.&lt;/li&gt;
&lt;li&gt;Replaced the pseudo-tag from MFCCs to Chroma music features for harmonic information.&lt;/li&gt;
&lt;li&gt;Utilising deep features like Encodec instead of k-means for scaling up models to 1 B parameters.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Time-Variant Reverberation Algorithm for Reverberation Enhancement Syetem</title>
      <link>/project/reverb/</link>
      <pubDate>Fri, 12 May 2023 22:05:06 -0400</pubDate>
      <guid>/project/reverb/</guid>
      <description>&lt;p&gt;The reverberation algorithm is usually an LTI system. The room in the concert hall does not
change, so the response does not change over time. However, this can lead to colouration and
instability due to feedback caused by the close proximity of the microphones and speakers. Some
time-varying systems can solve this problem. This paper implements a new time-varying variable
reverberation algorithm on bela, based on a combination of delay-line, lowpass filter and comb
filter and feedback, for reverberation enhancement systems. Such systems can often be used in
electroacoustically enhanced rehearsal rooms. This particular application is briefly outlined, and
other possible applications are discussed while the shortcomings of the experimental approach are
analysed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#39;../../files/map.mp4&#39; target=&#39;_blank&#39;&gt;Demo recoding&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised TrainingCCF none</title>
      <link>/project/tmnn/</link>
      <pubDate>Mon, 29 Aug 2022 21:40:17 +0100</pubDate>
      <guid>/project/tmnn/</guid>
      <description>&lt;p&gt;09/2021 – 07/2022&lt;/p&gt;
&lt;p&gt;Research Assistant, Supervised by Prof. Richard Stern, Carnegie Mellon University&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constructed 2-layer learnable front ends in Temporal Modulation Neural Network (TMNN) that combines
Mel-like data-driven front ends and temporal modulation filters.&lt;/li&gt;
&lt;li&gt;Examined the proposed front ends surpass state-of-the-art (SOTA) methods on the MagnaTagATune dataset in
automatic music tagging, and they are also helpful for keyword spotting on speech commands.&lt;/li&gt;
&lt;li&gt;Analysis of the model performance among tags with different genres and instrument tags.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learnable Frontend for Music, Speech and Audiow</title>
      <link>/project/masterthesis/</link>
      <pubDate>Tue, 02 Nov 2021 22:05:06 -0400</pubDate>
      <guid>/project/masterthesis/</guid>
      <description>&lt;p&gt;Design learnable frontends for deep learning models inspired by classic filters, multi-rate sampling &amp;amp; modulation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cover Song Detection &amp; Evaluation of Automatic Speech Recognition</title>
      <link>/project/teencent2021/</link>
      <pubDate>Tue, 02 Nov 2021 21:49:27 -0400</pubDate>
      <guid>/project/teencent2021/</guid>
      <description>&lt;p&gt;May 2020 &amp;ndash; Aug. 2021. Beijing, CHN. Summer internship in Tencent Holdings Limited. (Beijing)&lt;/p&gt;
&lt;p&gt;Write literature review on coversong detection.&lt;/p&gt;
&lt;p&gt;Reproduced and refined music separation &amp;amp; covering detection.&lt;/p&gt;
&lt;p&gt;Evaluating different ASR models on business data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tempo Detection of Chinese Pop Music</title>
      <link>/project/summer_deepmusic_mir/</link>
      <pubDate>Wed, 02 Sep 2020 00:21:40 +0800</pubDate>
      <guid>/project/summer_deepmusic_mir/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Jun. 2020 &amp;ndash; Sept. 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;Summer internship in Beijing Deepmusic Technology Co. LTD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Write literature review on song tempo/speed detection.&lt;/p&gt;
&lt;p&gt;Designing new model on tempo detection based on BiLSTM and Temporal Convolution Network(TCN) and compared them with the baselines of Librosa and MadMOM using the data provided by Renren Karaoke Company (more than 2000 songs manually marked by my colleagues).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the music with stable speed or with or a slightly slower ending, the accuracy of tempo recognition is above 87% without considering the double frequency (error is less than or equal to 0.01) and above 98% general accuracy (error is less than 0.1). While MadMOM less than 90%.&lt;/li&gt;
&lt;li&gt;The accuracy in distinguishing between three beats(three four beats, six eight beats, twelvw eight beats etc.) and four beats(four four beats etc.) is 95 percent.&lt;/li&gt;
&lt;li&gt;The local tempo tasks such as song&#39;s inherent shift and whether the ending slows down deserve more attention&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Research &amp; implementation of Chinese flute playing technique recognition based on machine learning</title>
      <link>/project/graduate/</link>
      <pubDate>Wed, 02 Sep 2020 00:21:10 +0800</pubDate>
      <guid>/project/graduate/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Feb. 2020 – May 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;One of the graduation theses that awarded the outstanding &lt;a href=&#39;../../files/毕设论文.pdf&#39; target=&#39;_blank&#39;&gt;paper&lt;/a&gt; honor of School of Mathematical Science, Peking University.&lt;/li&gt;
&lt;li&gt;Research Assistant for prof. CHEN Xiaoou in Wangxuan Institute of Computer Technology at Peking University.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Music object recognition and recording is the essential component of music information retrieval. Different from other fields of melody extraction and music transcription, the research on musical instrument technique detection is still in the early stage. The existing work mainly focuses on technique detection of individual notes or frame, and there is a lack of effective data set.&lt;/p&gt;
&lt;p&gt;This article constructed &lt;a href=&#39;https://pan.baidu.com/s/1Czf6ZYqkW1EEpZeJh_3_gw&#39; target=&#39;_blank&#39;&gt;audio dataset&lt;/a&gt;(extraction code &amp;ldquo;jvbk&amp;rdquo; if needed) on ten kinds of techniques of transverse Chinese bamboo flute (Di), with 101 minutes of audio, using Mel frequency spectrum as audio feature, and put forward a model based on the fully convolutional neural network (FCNN). This model is an end-to-end sound event detector for variable length of the input and can be used in the detection of
instruments technology.&lt;/p&gt;
&lt;p&gt;Compared to the model based on VGG13, VGG16 and LeNet-5 baseline to evaluate the effectiveness of the proposed framework, this model got the average accuracy 94%, much better than all the others, on input of the different length of audio, which suggest good generalization ability.&lt;/p&gt;
&lt;p&gt;Key Words： sound event detection, music information retrieval, fully convolutional neural network, musical instrumet technique detection&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TONE CONTOUR REALIZATION IN SUNG CANTONESE</title>
      <link>/project/phonetics/</link>
      <pubDate>Wed, 02 Sep 2020 00:19:18 +0800</pubDate>
      <guid>/project/phonetics/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Nov. 2019 &amp;ndash; Jan. 2020. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Class project&lt;/em&gt;, scored 92/100. Supervised by Associate prof. &lt;a href=&#39;https://chinese.pku.edu.cn/rwfc/1224333.htm&#39; target=&#39;_blank&#39;&gt;WANG Yunjia&lt;/a&gt;, department of Chinese Language and Literature, Peking University.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Final project of &lt;em&gt;Fundamental Chinese Phonetics&lt;/em&gt; course: A brief discussion on the relationship between the basic frequency and the lyric tone of Cantonese songs (class final paper see at &lt;a href=&#39;../../files/略论粤语歌曲基频与歌词声调的关联.pdf&#39; target=&#39;_blank&#39;&gt;略论粤语歌曲基频与歌词声调的关联&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Based on &lt;a href=&#39;https://pan.baidu.com/s/1hjZrIAGYvPX7zviB87aeFA&#39; target=&#39;_blank&#39;&gt;acoustic phonetics data&lt;/a&gt;(extraction code &amp;ldquo;uwr8&amp;rdquo; if needed), this paper discusses the influence of the fundamental frequency of cantonese songs on the tone of the lyrics. Eleven phonators (6 female and 5 male) aged between 18 and 22 years old were analyzed for the singing and lyric reading sound of the designated song, to explore the change of fundamental frequency over time, and the study tone involved four of the six tones in Cantonese.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;paires t-test&lt;/em&gt; of the acoustic data showed that :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there was a significant difference in the slope of the fundamental frequency of lyric reading over time in tones with different tone values;&lt;/li&gt;
&lt;li&gt;There is no significant difference in the slope of the fundamental frequency of the song over time between the lyrics of each tone;&lt;/li&gt;
&lt;li&gt;There is a certain relationship between the basic frequency slope of each tone and the basic frequency slope of the song.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Chinese instrument recognition</title>
      <link>/project/csmt2019_pub/</link>
      <pubDate>Tue, 01 Sep 2020 23:47:26 +0800</pubDate>
      <guid>/project/csmt2019_pub/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Mar 2019 – Jun 2019. Beijing, CHN.&lt;/li&gt;
&lt;li&gt;Research Assistant for prof. CHEN Xiaoou in Wangxuan Institute of Computer Technology at Peking University.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Main Information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up a series of quartet database from DCMI database shared by China Conservatory of Music.&lt;/li&gt;
&lt;li&gt;Constructed an audio event detection model based on CRNN to detect and recognize instruments.&lt;/li&gt;
&lt;li&gt;Evaluated the percussion, recall rate and F-measure of the model and CNN baseline model, and compared the difference among different quartet databases generate from different music skills or music types.&lt;/li&gt;
&lt;li&gt;published &lt;a href=&#39;../../files/publication.pdf&#39; target=&#39;_blank&#39;&gt;result&lt;/a&gt; on Conference of Sound and Music Technology 2019.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Correspondence between Speech Melody and Pitch Contour in Sichuan Folk Song</title>
      <link>/project/duan_zhiyao/</link>
      <pubDate>Mon, 02 Sep 2019 00:19:03 +0800</pubDate>
      <guid>/project/duan_zhiyao/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Jul. 2019 – Sept. 2019. Rochester, NY, US.&lt;/li&gt;
&lt;li&gt;Research Assistant supervised by &lt;a href=&#39;http://www2.ece.rochester.edu/~zduan/&#39; target=&#39;_blank&#39;&gt;prof. DUAN Zhiyao&lt;/a&gt;, Deparment of Electronic Computer Engineer, University of Rochester.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Main Information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Made a literature review of linguistics papers on relations between speech melody and notes in tone language song.&lt;/li&gt;
&lt;li&gt;Set up a database on Sichuan folk song with music scores in MusicXML form, lyrics audio in Sichuan dialect in wav form and note-audio alignment character by character.&lt;/li&gt;
&lt;li&gt;Analyzed the correspondence among the tone of each character, the change of fundamental frequency of each character audio and the change of music notes linked to it.&lt;/li&gt;
&lt;li&gt;Evaluated different correspondence rates in different type of folk songs&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
